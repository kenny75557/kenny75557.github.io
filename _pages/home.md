---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /home/
  - /home.html
---

Hi, I'm Yen-Hsiang Wang (王彥翔). Currently as Research Assitant at the [Natural Language Processing Lab](https://nlpnchu.org/) at National Chung Hsing University. My master's research is advised by Professor [Yao-Chang Fan](https://yfan.nlpnchu.org/). 

My recent research and projects are as follows include Natural Language Processing and Machine Learning, including but not limited to:

- Exploring how LLMs can enhance translation for low-resource or unseen languages by leveraging Chain-of-Thought reasoning and providing relevant patterns through sentence retrieval.
- Distilling knowledge from dense embedding models for query expansion finetuning to enhance the performance of term-based retrieval systems.

[Download my CV here](/files/heliart_CV.pdf)

<!-- Currently seeking for PhD opportunities starting from 2025 Fall. -->

## Research Interests
* Reliable and robust integration of effective retrievers and large language models.
* Interpretability on Large Language Models.
* Human collaboration on LLM Agents and robustness of Reasoning Agents.

I am interested in enhancing the interpretability of model mechanisms and their understanding of human intentions, as well as in providing more accurate and reliable retrieval methods for Large Language Models.


# News
* 11/2024: I co-taught a tutorial on [Retrieval-Augmented Language Models](https://nlpnchu.org/rocling-tutorial-2024.github.io/) at [ROCLING 2024](https://nlpnchu.org/rocling-tutorial-2024.github.io/). Slides are available on our website.
* 11/2024: Our paper, which constructs a Cross-Lingual Retrieval Dataset in the Taiwan Legal domain, was nominated for Best Paper at [ROCLING 2024](https://nlpnchu.org/rocling-tutorial-2024.github.io/).
* 08/2024: Our papaer [Learning-From-Mistakes Prompting for Indigenous Language Translation](https://arxiv.org/abs/2410.11450) was accepted to the ACL LoResMT 2024.


# Service
Reviewer:
* GEM Workshop at EMNLP 2023
